Hadoop

Hadoop is a framework which provides processing of large dataset.

Hadoop mainly has two components:
1.Map Reduce-A programming model
2.HDFS-Distributed file system

Map Reduce has 2 phases:

1.map  phase
2.Reduce phase

In each phase I/p and O/p will be key,value pairs

Suppose data is like this:
name=bangalore_temp=12
name=mysore_temp=13
name=karnataka_temp=14
name=bangalore_temp=16
name=mysore_temp=10

*Map phase

Input to map phase will be key-offset(line-no),value-data in each line
Based on Map program suppose u extract name and temperature output will be like this:(key,value)

bangalore,12
mysore,13
karnataka,14
bangalore,16
mysore,10

*Mapreduce framework
This will sort the output of the map phase(run inbetween map and reduce phase)
bangalore [12,16]
mysore [10,13]

*Reduce phase
This will run the reduce program(supose max temp for each place) and produce the output as key,value pair
bangaore,16
mysore,13

Map Reduce has two trackers 

*Job tracker:It coordinates between map and reduce jobs by sheduling the tasks.
*Task tracker :This will track the task and give feedbask to job tracker.

HDFS

Large dataset input is divided into various splits with each split of 64 Mb

Map task is assigned on each split and for each record in the split(key,value) map program is run on them
Normally reduce task will be one which will take input from vaious mappers and reduce program is run on them.

HDFS creates many nodes with each node of size 64 Mb same as split size.So Map task runs on each node.

So This HDFS would divide the large data set in to many splits and each split is stored on a node.
Nodes are created across multiple machines(hadoop cluster).One Map task runs on each node.

HDFS has two nodes 

*namenode - It stores the metadata of the existing data.
*datanode - It stores the actual data.




